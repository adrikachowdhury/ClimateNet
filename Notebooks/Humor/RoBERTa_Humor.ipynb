{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQTp8yXPJ-f1"
      },
      "source": [
        "# **INSTALL AND IMPORT NECESSARY LIBRARIES**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_UPwVrCuIx9R",
        "outputId": "e2caf997-768d-4928-b235-ee3d87d90253"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.50.3)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Collecting unidecode\n",
            "  Downloading Unidecode-1.3.8-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting emoji\n",
            "  Downloading emoji-2.14.1-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting emoticon_fix\n",
            "  Downloading emoticon_fix-0.0.2.tar.gz (3.1 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.14.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.5.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m75.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m42.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Unidecode-1.3.8-py3-none-any.whl (235 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m235.5/235.5 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading emoji-2.14.1-py3-none-any.whl (590 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m590.6/590.6 kB\u001b[0m \u001b[31m39.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: emoticon_fix\n",
            "  Building wheel for emoticon_fix (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for emoticon_fix: filename=emoticon_fix-0.0.2-py3-none-any.whl size=3429 sha256=e313d46118b8e21fb1324c9691d37b5b52c29563a1b5f7d492464ea203aae8b4\n",
            "  Stored in directory: /root/.cache/pip/wheels/54/7d/29/b22990676d3b7251ac78b29ad05c827c6cbf8fd342a9d7133c\n",
            "Successfully built emoticon_fix\n",
            "Installing collected packages: xxhash, unidecode, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, fsspec, emoticon_fix, emoji, dill, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, nvidia-cusolver-cu12, datasets\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.5.0 dill-0.3.8 emoji-2.14.1 emoticon_fix-0.0.2 fsspec-2024.12.0 multiprocess-0.70.16 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 unidecode-1.3.8 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "# Install necessary packages\n",
        "!pip install transformers datasets torch scikit-learn nltk unidecode emoji emoticon_fix\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from transformers import RobertaTokenizer, RobertaForSequenceClassification, Trainer, TrainingArguments, DataCollatorWithPadding\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "from collections import Counter\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from unidecode import unidecode\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import emoji\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from transformers import EarlyStoppingCallback\n",
        "from emoticon_fix import emoticon_fix\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from imblearn.combine import SMOTEENN\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HaNbGk7mKET2"
      },
      "source": [
        "# **RoBERTa TRAINING WITH PREPROCESSINGS**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qAuZXD66iInt",
        "outputId": "4affb20f-b040-4dea-ea65-caf21481f94e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "# Download stopwords\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "snow_stemmer = SnowballStemmer(language='english')\n",
        "\n",
        "# Function to preprocess text\n",
        "def preprocess_text(text):\n",
        "    if pd.isna(text):  # Handle missing text\n",
        "        return \"\"\n",
        "    text = unidecode(text)  # Convert accented characters\n",
        "    text = emoji.demojize(text)  # Convert emojis\n",
        "    text = emoticon_fix.emoticon_fix(text)\n",
        "    words = text.split()\n",
        "    words = [snow_stemmer.stem(word) for word in words if word.lower() not in stop_words]\n",
        "    return \" \".join(words)\n",
        "\n",
        "# Load dataset\n",
        "file_path = \"climaconvo.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Rename columns\n",
        "df.rename(columns={'Tweet': 'text', 'Humor': 'label'}, inplace=True)\n",
        "\n",
        "df.dropna(subset=['label'], inplace=True)\n",
        "\n",
        "# Preprocess text\n",
        "df['text'] = df['text'].apply(preprocess_text)\n",
        "\n",
        "# Keep only classes\n",
        "df = df[df['label'].isin([0, 1])]\n",
        "\n",
        "# Train-test split\n",
        "df_train, df_test = train_test_split(df, test_size=0.2, stratify=df['label'], random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 807
        },
        "id": "GIjjeMIdeqVL",
        "outputId": "b2e282e7-aceb-42a5-9d43-6383b6d4a370",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-4-05d23224ea71>:85: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `WeightedTrainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = WeightedTrainer(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2565' max='2565' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2565/2565 16:14, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.618400</td>\n",
              "      <td>0.560336</td>\n",
              "      <td>0.779539</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.483800</td>\n",
              "      <td>0.376073</td>\n",
              "      <td>0.819404</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.365800</td>\n",
              "      <td>0.315649</td>\n",
              "      <td>0.853026</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.318100</td>\n",
              "      <td>0.279407</td>\n",
              "      <td>0.875120</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.342900</td>\n",
              "      <td>0.243348</td>\n",
              "      <td>0.892891</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generating Classification Reports...\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Set Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.88      0.91      0.89      8195\n",
            "         1.0       0.90      0.88      0.89      8195\n",
            "\n",
            "    accuracy                           0.89     16390\n",
            "   macro avg       0.89      0.89      0.89     16390\n",
            "weighted avg       0.89      0.89      0.89     16390\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation Set Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.99      0.90      0.94      2049\n",
            "         1.0       0.08      0.55      0.14        33\n",
            "\n",
            "    accuracy                           0.89      2082\n",
            "   macro avg       0.54      0.72      0.54      2082\n",
            "weighted avg       0.98      0.89      0.93      2082\n",
            "\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Load tokenizer\n",
        "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
        "\n",
        "# Define dataset class\n",
        "class ClimateDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer):\n",
        "        self.encodings = tokenizer(list(texts), padding=True, truncation=True, max_length=256, return_tensors=\"pt\")\n",
        "        self.labels = torch.tensor(list(labels), dtype=torch.long)  # FIXED: Convert Series to list\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
        "        item[\"labels\"] = self.labels[idx]\n",
        "        return item\n",
        "\n",
        "# Load data (Ensure df_train and df_test are defined)\n",
        "X_train = df_train[['text']]\n",
        "y_train = df_train['label']\n",
        "\n",
        "# Apply oversampling to balance the dataset\n",
        "ros = RandomOverSampler(sampling_strategy=\"auto\", random_state=42)\n",
        "X_train_balanced, y_train_balanced = ros.fit_resample(X_train, y_train)\n",
        "\n",
        "# Convert to lists for dataset creation\n",
        "X_train_balanced = X_train_balanced['text'].tolist()\n",
        "y_train_balanced = y_train_balanced.tolist()\n",
        "\n",
        "# Compute class weights AFTER balancing\n",
        "class_weights = compute_class_weight(class_weight=\"balanced\", classes=np.unique(y_train_balanced), y=y_train_balanced)\n",
        "class_weights = torch.tensor(class_weights, dtype=torch.float32)\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = ClimateDataset(X_train_balanced, y_train_balanced, tokenizer)\n",
        "val_dataset = ClimateDataset(df_test['text'], df_test['label'], tokenizer)\n",
        "\n",
        "# Load pre-trained roberta model with increased dropout\n",
        "model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=2)\n",
        "model.config.hidden_dropout_prob = 0.3  # More dropout for generalization\n",
        "\n",
        "# Define optimizer\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=3e-4, momentum = 0.9)\n",
        "\n",
        "# Define custom Trainer with Weighted Loss\n",
        "class WeightedTrainer(Trainer):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
        "        labels = inputs[\"labels\"]\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "\n",
        "        loss_fct = nn.CrossEntropyLoss(weight=class_weights.to(logits.device))\n",
        "        loss = loss_fct(logits, labels)\n",
        "\n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "# Define evaluation metrics\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=1)\n",
        "    return {\"accuracy\": accuracy_score(labels, predictions)}\n",
        "\n",
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=32,\n",
        "    num_train_epochs=5,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=10,\n",
        "    save_total_limit=2,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"accuracy\",\n",
        "    greater_is_better=True,\n",
        "    fp16=True,  # Mixed precision for faster training\n",
        "    report_to=\"none\",\n",
        "    save_on_each_node=False,\n",
        "    disable_tqdm=False\n",
        ")\n",
        "\n",
        "# Trainer\n",
        "trainer = WeightedTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        "    optimizers=(optimizer, None),\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]  # Stop if no improvement in 3 epochs\n",
        ")\n",
        "\n",
        "# Train model\n",
        "trainer.train()\n",
        "\n",
        "# Evaluate model\n",
        "print(\"\\nGenerating Classification Reports...\\n\" + \"-\" * 50)\n",
        "\n",
        "# Train Set Classification Report\n",
        "train_preds = trainer.predict(train_dataset).predictions\n",
        "train_preds = np.argmax(train_preds, axis=1)\n",
        "print(\"\\nTrain Set Classification Report:\")\n",
        "print(classification_report(y_train_balanced, train_preds))\n",
        "\n",
        "# Validation Set Classification Report\n",
        "test_preds = trainer.predict(val_dataset).predictions\n",
        "test_preds = np.argmax(test_preds, axis=1)\n",
        "print(\"\\nValidation Set Classification Report:\")\n",
        "print(classification_report(df_test['label'].tolist(), test_preds))\n",
        "\n",
        "print(\"-\" * 50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cOY9IqyqJPC5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "deb81f31-3c11-4a56-c8ad-4da275973e8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/12], Loss: 0.0980\n",
            "Epoch [2/12], Loss: 0.0128\n",
            "Epoch [3/12], Loss: 0.0068\n",
            "Epoch [4/12], Loss: 0.0046\n",
            "Epoch [5/12], Loss: 0.0043\n",
            "Epoch [6/12], Loss: 0.0029\n",
            "Epoch [7/12], Loss: 0.0024\n",
            "Epoch [8/12], Loss: 0.0033\n",
            "Epoch [9/12], Loss: 0.0037\n",
            "Epoch [10/12], Loss: 0.0038\n",
            "Epoch [11/12], Loss: 0.0033\n",
            "Epoch [12/12], Loss: 0.0022\n",
            "\n",
            "Validation Set Classification Report (After Nested BiLSTM):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      1.00      0.99      2049\n",
            "           1       0.10      0.03      0.05        33\n",
            "\n",
            "    accuracy                           0.98      2082\n",
            "   macro avg       0.54      0.51      0.52      2082\n",
            "weighted avg       0.97      0.98      0.98      2082\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Extract features from roberta\n",
        "def extract_roberta_features(model, dataloader, device):\n",
        "    model.eval()\n",
        "    features = []\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            outputs = model(input_ids, attention_mask=attention_mask, output_hidden_states=True)\n",
        "            hidden_states = outputs.hidden_states[-1]  # Last hidden layer\n",
        "            features.append(hidden_states[:, 0, :].cpu().numpy())  # CLS token representation\n",
        "    return np.vstack(features)\n",
        "\n",
        "# Prepare DataLoader\n",
        "batch_size = 32\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Load distilbertT model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Extract features\n",
        "train_features = extract_roberta_features(model, train_dataloader, device)\n",
        "val_features = extract_roberta_features(model, val_dataloader, device)\n",
        "\n",
        "# Normalize features\n",
        "scaler = StandardScaler()\n",
        "train_features = scaler.fit_transform(train_features)\n",
        "val_features = scaler.transform(val_features)\n",
        "\n",
        "# --------------------------------\n",
        "# ğŸ”¹ Define Nested BiLSTM Model\n",
        "# --------------------------------\n",
        "\n",
        "class NestedBiLSTM(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers, dropout):\n",
        "        super(NestedBiLSTM, self).__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "        # First BiLSTM layer\n",
        "        self.bilstm1 = nn.LSTM(input_dim, hidden_dim, num_layers=1, batch_first=True, dropout=dropout, bidirectional=True)\n",
        "\n",
        "        # Subsequent BiLSTM layers\n",
        "        self.bilstm_layers = nn.ModuleList([\n",
        "            nn.LSTM(hidden_dim * 2, hidden_dim, num_layers=1, batch_first=True, dropout=dropout, bidirectional=True)\n",
        "            for _ in range(num_layers - 1)\n",
        "        ])\n",
        "\n",
        "        # Fully connected layer (adjusted for bidirectional hidden size)\n",
        "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Pass through first BiLSTM layer\n",
        "        out, _ = self.bilstm1(x)\n",
        "\n",
        "        # Pass through subsequent BiLSTM layers (nested)\n",
        "        for bilstm in self.bilstm_layers:\n",
        "            out, _ = bilstm(out)\n",
        "\n",
        "        # Get the last time-step output\n",
        "        out = out[:, -1, :]  # Take the last time step\n",
        "\n",
        "        # Fully connected layer\n",
        "        output = self.fc(out)\n",
        "        return output\n",
        "\n",
        "# --------------------------------\n",
        "# ğŸ”¹ Define Dataset Class for LSTM\n",
        "# --------------------------------\n",
        "\n",
        "class LSTMFeatureDataset(Dataset):\n",
        "    def __init__(self, features, labels):\n",
        "        # Add an extra dimension for sequence length (1 in this case)\n",
        "        self.features = torch.tensor(features, dtype=torch.float32).unsqueeze(1)  # Shape: (batch_size, 1, input_dim)\n",
        "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.features[idx], self.labels[idx]\n",
        "\n",
        "# Prepare Dataset for Nested BiLSTM\n",
        "train_lstm_dataset = LSTMFeatureDataset(train_features, y_train_balanced)\n",
        "val_lstm_dataset = LSTMFeatureDataset(val_features, df_test['label'].tolist())\n",
        "\n",
        "train_loader = DataLoader(train_lstm_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_lstm_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Define parameters for Nested BiLSTM\n",
        "bilstm_input_dim = train_features.shape[1]  # Feature size\n",
        "bilstm_hidden_dim = 256\n",
        "bilstm_output_dim = 2\n",
        "bilstm_num_layers = 1  # Number of BiLSTM layers (nested)\n",
        "bilstm_dropout = 0.5\n",
        "\n",
        "# Initialize Nested BiLSTM model\n",
        "nested_bilstm_model = NestedBiLSTM(bilstm_input_dim, bilstm_hidden_dim, bilstm_output_dim, bilstm_num_layers, bilstm_dropout).to(device)\n",
        "\n",
        "# Define optimizer & loss function for BiLSTM\n",
        "bilstm_criterion = nn.CrossEntropyLoss()\n",
        "bilstm_optimizer = torch.optim.Adam(nested_bilstm_model.parameters(), lr=5e-4)\n",
        "\n",
        "# Train Nested BiLSTM model\n",
        "for epoch in range(12):\n",
        "    nested_bilstm_model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch_features, batch_labels in train_loader:\n",
        "        batch_features, batch_labels = batch_features.to(device), batch_labels.to(device)\n",
        "\n",
        "        bilstm_optimizer.zero_grad()\n",
        "        outputs = nested_bilstm_model(batch_features)\n",
        "        loss = bilstm_criterion(outputs, batch_labels)\n",
        "        loss.backward()\n",
        "        bilstm_optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/12], Loss: {total_loss / len(train_loader):.4f}\")\n",
        "\n",
        "# --------------------------------\n",
        "# ğŸ”¹ Evaluate Nested BiLSTM Model\n",
        "# --------------------------------\n",
        "\n",
        "nested_bilstm_model.eval()\n",
        "all_features, all_labels = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch_features, batch_labels in val_loader:\n",
        "        batch_features, batch_labels = batch_features.to(device), batch_labels.to(device)\n",
        "\n",
        "        # Get Nested BiLSTM output (features before classification layer)\n",
        "        outputs = nested_bilstm_model(batch_features)\n",
        "\n",
        "        # Collect features and labels\n",
        "        all_features.extend(outputs.cpu().numpy())\n",
        "        all_labels.extend(batch_labels.cpu().numpy())\n",
        "\n",
        "# Print classification report after Nested BiLSTM\n",
        "print(\"\\nValidation Set Classification Report (After Nested BiLSTM):\")\n",
        "print(classification_report(all_labels, np.argmax(all_features, axis=1)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTWl7pigT567"
      },
      "source": [
        "# **MLP**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hmV-LYnLJQ95",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c7ad5832-bde9-4d9f-94e7-62e2e75a98b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Validation Set Classification Report for MLP:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      1.00      0.99      2049\n",
            "           1       1.00      0.03      0.06        33\n",
            "\n",
            "    accuracy                           0.98      2082\n",
            "   macro avg       0.99      0.52      0.53      2082\n",
            "weighted avg       0.98      0.98      0.98      2082\n",
            "\n",
            "Predictions saved as humor_predictions_roberta.csv!\n"
          ]
        }
      ],
      "source": [
        "# Train MLP on balanced data\n",
        "mlp_classifier = MLPClassifier(hidden_layer_sizes=(1024, 512, 256), max_iter=1000, random_state=42, solver='adam', learning_rate_init=2e-3)\n",
        "mlp_classifier.fit(all_features, all_labels)\n",
        "\n",
        "# Predictions from MLP\n",
        "y_pred_mlp = mlp_classifier.predict(all_features)\n",
        "\n",
        "# Classification Report\n",
        "print(\"\\nValidation Set Classification Report for MLP:\")\n",
        "print(classification_report(all_labels, y_pred_mlp))\n",
        "\n",
        "# Save Predictions to CSV\n",
        "df_predictions = pd.DataFrame({\n",
        "    \"Actual\": all_labels,\n",
        "    \"Predicted\": y_pred_mlp\n",
        "})\n",
        "prediction_file_path = \"humor_predictions_roberta.csv\"\n",
        "df_predictions.to_csv(prediction_file_path, index=False)\n",
        "\n",
        "print(f\"Predictions saved as {prediction_file_path}!\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}